# üóÑÔ∏è Optimisation Base de Donn√©es ‚Äî Diagnostic et Solutions

**Date** : 2026-02-16  
**Phase** : Phase 6  
**Objectif** : Am√©liorer les performances de la base de donn√©es SQLite (requ√™tes, index, connexions)

---

## üìä 1. Diagnostic

### üî¥ Probl√®me Critique : Surcharge de Connexions

**Observation** :
```python
# db.py ‚Äî Pattern actuel (PROBL√àME)
def query_kwic(self, term: str, ...) -> list[KwicHit]:
    conn = self._conn()  # ‚ùå Nouvelle connexion
    try:
        return _query_kwic(conn, ...)
    finally:
        conn.close()  # ‚ùå Fermeture imm√©diate
```

**Impact** :
- **60+ m√©thodes** ouvrent/ferment des connexions individuellement
- Une recherche KWIC + affichage segments = **5-10 connexions**
- Refresh UI (arbre corpus) = **20-50 connexions** selon le nombre d'√©pisodes

**Calcul de surcharge** :
- Ouverture connexion SQLite : ~2-5ms
- Refresh UI avec 50 √©pisodes : **100-250ms** juste pour les connexions !

---

### üü° Index Manquants ou Sous-Optimis√©s

#### Requ√™tes fr√©quentes non index√©es :

1. **Filtrage par statut d'√©pisode** :
```sql
SELECT * FROM episodes WHERE status = ?  -- Pas d'index sur status
```

2. **Recherche segments par speaker** :
```sql
SELECT DISTINCT speaker_explicit FROM segments WHERE ...  -- Lent
```

3. **Comptage cues par langue** :
```sql
SELECT COUNT(*) FROM subtitle_cues WHERE lang = ?  -- Pas d'index d√©di√©
```

#### Index existants (002_segments.sql) :
‚úÖ `idx_segments_episode_kind_n` sur `(episode_id, kind, n)`  
‚úÖ `idx_subtitle_tracks_episode` sur `(episode_id)`  
‚úÖ `idx_subtitle_cues_episode_lang` sur `(episode_id, lang)`  
‚úÖ `idx_align_links_run` sur `(align_run_id)`

---

### üü° Transactions Non-Optimales

**Probl√®me** : Insertions multiples sans transaction explicite

```python
# db_segments.py ‚Äî CORRECT ‚úÖ
def upsert_segments(conn, episode_id, kind, segments):
    with conn:  # Transaction automatique
        conn.execute("DELETE FROM segments WHERE ...")
        for seg in segments:  # 1 transaction pour tous
            conn.execute("INSERT INTO segments ...")
```

Mais dans `db.py`, les appels successifs ouvrent/ferment :
```python
# ‚ùå Pattern probl√©matique
for ep in episodes:
    db.upsert_episode(ep)  # Connexion 1, 2, 3...
```

---

## üöÄ 2. Solutions Propos√©es

### Solution 1 : Context Manager pour Connexions

**Objectif** : R√©utiliser une connexion pour plusieurs op√©rations

```python
@contextmanager
def connection(self) -> sqlite3.Connection:
    """Context manager pour r√©utiliser une connexion."""
    conn = self._conn()
    try:
        yield conn
    finally:
        conn.close()

# Utilisation :
with db.connection() as conn:
    db_segments.upsert_segments(conn, ep_id, "sentence", segs)
    db_segments.upsert_segments(conn, ep_id, "utterance", utts)
    # 1 seule connexion pour 2+ op√©rations !
```

**Avantage** : 
- R√©trocompatible avec l'API existante
- Optionnel (on peut garder les m√©thodes simples)
- R√©duit la surcharge de **90%** pour les op√©rations batch

---

### Solution 2 : M√©thodes Batch

```python
def upsert_episodes_batch(self, refs: list[EpisodeRef], status: str = "new") -> None:
    """Ins√®re plusieurs √©pisodes en une seule transaction."""
    conn = self._conn()
    try:
        with conn:
            for ref in refs:
                conn.execute(
                    """INSERT INTO episodes (...)
                       VALUES (?, ?, ?, ?, ?, ?)
                       ON CONFLICT(episode_id) DO UPDATE SET ...""",
                    (ref.episode_id, ref.season, ref.episode, ref.title, ref.url, status),
                )
    finally:
        conn.close()
```

---

### Solution 3 : Index Additionnels

**Migration `005_optimize_indexes.sql`** :

```sql
-- Index sur status pour filtrage rapide
CREATE INDEX IF NOT EXISTS idx_episodes_status ON episodes(status);

-- Index composite season+episode pour recherche rapide
CREATE INDEX IF NOT EXISTS idx_episodes_season_episode ON episodes(season, episode);

-- Index sur speaker_explicit pour recherche locuteurs
CREATE INDEX IF NOT EXISTS idx_segments_speaker ON segments(speaker_explicit) 
  WHERE speaker_explicit IS NOT NULL;

-- Index sur lang pour comptage rapide des sous-titres
CREATE INDEX IF NOT EXISTS idx_subtitle_cues_lang ON subtitle_cues(lang);

-- Index composite pour requ√™tes d'alignement fr√©quentes
CREATE INDEX IF NOT EXISTS idx_align_links_episode_status ON align_links(episode_id, status);

UPDATE schema_version SET version = 5;
```

**Impact attendu** :
- Filtrage par statut : **10-50x plus rapide** (scan ‚Üí index)
- Recherche locuteurs : **5-20x plus rapide**
- Comptage sous-titres : **instantan√©**

---

### Solution 4 : Optimisations SQLite

**Pragma au d√©marrage** :

```python
def _conn(self) -> sqlite3.Connection:
    conn = sqlite3.connect(self.db_path)
    # Optimisations de performance
    conn.execute("PRAGMA journal_mode = WAL")  # Write-Ahead Logging
    conn.execute("PRAGMA synchronous = NORMAL")  # Balance s√©curit√©/perf
    conn.execute("PRAGMA cache_size = -64000")  # Cache 64MB
    conn.execute("PRAGMA temp_store = MEMORY")  # Temp tables en RAM
    conn.execute("PRAGMA mmap_size = 268435456")  # Memory-mapped I/O 256MB
    return conn
```

**Justification** :
- **WAL mode** : Lectures concurrentes + √©critures non-bloquantes
- **cache_size** : R√©duit les I/O disque (important pour FTS5)
- **mmap_size** : Acc√©l√®re les grosses requ√™tes KWIC

---

## üìà 3. Gains de Performance Attendus

### Benchmark Estim√©

| Op√©ration | Avant | Apr√®s | Gain |
|-----------|-------|-------|------|
| Refresh UI (50 √©pisodes) | ~200ms | ~20ms | **10x** |
| Recherche KWIC (1000 hits) | ~150ms | ~80ms | **2x** |
| Import 10 √©pisodes | ~500ms | ~100ms | **5x** |
| Filtrage par statut | ~50ms | ~2ms | **25x** |
| Comptage segments/cues | ~30ms | ~3ms | **10x** |

**Gain global estim√©** : **5-10x sur op√©rations UI courantes**

---

## ‚úÖ 4. Plan d'Impl√©mentation

1. ‚úÖ **Documenter** le diagnostic (ce fichier)
2. ‚è≥ **Cr√©er migration** `005_optimize_indexes.sql`
3. ‚è≥ **Ajouter PRAGMA** dans `_conn()`
4. ‚è≥ **Impl√©menter `connection()` context manager**
5. ‚è≥ **Ajouter m√©thodes batch** (`upsert_episodes_batch`, etc.)
6. ‚è≥ **Tester performances** avec benchmark script
7. ‚è≥ **Documenter changements** dans `CHANGELOG_DB_PHASE6.md`

---

## üîç 5. Tests de Validation

### Test 1 : V√©rifier les index

```sql
-- V√©rifier qu'un index est utilis√©
EXPLAIN QUERY PLAN 
SELECT * FROM episodes WHERE status = 'indexed';
-- Doit afficher : SEARCH episodes USING INDEX idx_episodes_status
```

### Test 2 : Benchmark connexions

```python
import time

# Avant (pattern actuel)
start = time.perf_counter()
for i in range(100):
    conn = db._conn()
    conn.execute("SELECT 1")
    conn.close()
elapsed_before = time.perf_counter() - start

# Apr√®s (context manager)
start = time.perf_counter()
with db.connection() as conn:
    for i in range(100):
        conn.execute("SELECT 1")
elapsed_after = time.perf_counter() - start

print(f"Avant : {elapsed_before:.3f}s")
print(f"Apr√®s : {elapsed_after:.3f}s")
print(f"Gain  : {elapsed_before / elapsed_after:.1f}x")
```

---

## üìö R√©f√©rences

- [SQLite Performance Tuning](https://www.sqlite.org/pragma.html#pragma_optimize)
- [FTS5 Best Practices](https://www.sqlite.org/fts5.html)
- [Write-Ahead Logging](https://www.sqlite.org/wal.html)
